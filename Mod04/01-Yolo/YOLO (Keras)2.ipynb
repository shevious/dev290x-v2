{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Object Detection with YOLO in Keras\n\nIn this exercise, you'll use a Keras implementation of YOLO to detect objects in an image.\n\n> **Important**: Using the YOLO model is resource-intensive. before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook.\n\n## Install Keras\nFirst, let's ensure that the latest version of Keras is installed."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download and Convert YOLO weights\nYOLO is based on the Darknet model architecture - an open-source model written in C. The creators of this model have provided pre-trained weights that were trained on the [Common Objects in Context (COCO) dataset](cocodataset.org) - a common set of sample images for computer vision research.\n\nRun the following cell to download the weights, and convert them into a suitable format for use with Keras.\n\n> _**Note**: This can take some time to run_"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "!wget https://pjreddie.com/media/files/yolov3.weights -O ~/yolo3.weights\n!python yolo_keras/convert.py yolo_keras/yolov3.cfg ~/yolo3.weights ~/yolo.h5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After the weights have been downloaded, the layers in the model are described.\n\n## Load the Weights into a Keras Model\n\nNow that we have the weights, we can load them into a Keras model.\n\n> **Note** The code to implement the Keras model is in **yolo_keras/model.py**. Additionally, **yolo_keras/utils.py** contains some functions that are used to assemble and use the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.layers import Input\n\nfrom yolo_keras.utils import *\nfrom yolo_keras.model import *\n\n# Get the COCO classes on which the model was trained\nclasses_path = \"yolo_keras/coco_classes.txt\"\nwith open(classes_path) as f:\n    class_names = f.readlines()\n    class_names = [c.strip() for c in class_names] \nnum_classes = len(class_names)\n\n# Get the anchor box coordinates for the model\nanchors_path = \"yolo_keras/yolo_anchors.txt\"\nwith open(anchors_path) as f:\n    anchors = f.readline()\n    anchors = [float(x) for x in anchors.split(',')]\n    anchors = np.array(anchors).reshape(-1, 2)\nnum_anchors = len(anchors)\n\n# Set the expected image size for the model\nmodel_image_size = (416, 416)\n\n# Create YOLO model\nhome = os.path.expanduser(\"~\")\nmodel_path = os.path.join(home, \"yolo.h5\")\nyolo_model = load_model(model_path, compile=False)\n\n# Generate output tensor targets for bounding box predictions\n# Predictions for individual objects are based on a detection probability threshold of 0.3\n# and an IoU threshold for non-max suppression of 0.45\ninput_image_shape = K.placeholder(shape=(2, ))\nboxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape,\n                                    score_threshold=0.3, iou_threshold=0.45)\n\nprint(\"YOLO model ready!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the Model to Detect Objects\n\nNow we're ready to use the YOLO model to detect objects in images.\n\n### Create functions to detect and display objects\nWe'll create a couple of functions:\n\n- **detect_objects**: Submits an image to the model and returns predicted object locations\n- **show_objects**: Displays the image with a bounding box fo each detected object."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def detect_objects(image):\n    \n    # normalize and reshape image data\n    image_data = np.array(image, dtype='float32')\n    image_data /= 255.\n    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n\n    # Predict classes and locations using Tensorflow session\n    sess = K.get_session()\n    out_boxes, out_scores, out_classes = sess.run(\n                [boxes, scores, classes],\n                feed_dict={\n                    yolo_model.input: image_data,\n                    input_image_shape: [image.size[1], image.size[0]],\n                    K.learning_phase(): 0\n                })\n    return out_boxes, out_scores, out_classes\n\ndef show_objects(image, out_boxes, out_scores, out_classes):\n    import random\n    from PIL import Image\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n\n    %matplotlib inline \n    \n    # Set up some display formatting\n    cmap = plt.get_cmap('tab20b')\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    # Plot the image\n    img = np.array(image)\n    plt.figure()\n    fig, ax = plt.subplots(1, figsize=(12,9))\n    ax.imshow(img)\n\n    # Set up padding for boxes\n    img_size = model_image_size[0]\n    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n    unpad_h = img_size - pad_y\n    unpad_w = img_size - pad_x\n\n    # Use a random color for each class\n    unique_labels = np.unique(out_classes)\n    n_cls_preds = len(unique_labels)\n    bbox_colors = random.sample(colors, n_cls_preds)\n\n    # process each instance of each class that was found\n    for i, c in reversed(list(enumerate(out_classes))):\n\n        # Get the class name\n        predicted_class = class_names[c]\n        # Get the box coordinate and probability score for this instance\n        box = out_boxes[i]\n        score = out_scores[i]\n\n        # Format the label to be added to the image for this instance\n        label = '{} {:.2f}'.format(predicted_class, score)\n\n        # Get the box coordinates\n        top, left, bottom, right = box\n        y1 = max(0, np.floor(top + 0.5).astype('int32'))\n        x1 = max(0, np.floor(left + 0.5).astype('int32'))\n        y2 = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n        x2 = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n\n        # Set the box dimensions\n        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n        \n        # Add a box with the color for this class\n        color = bbox_colors[int(np.where(unique_labels == c)[0])]\n        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n        ax.add_patch(bbox)\n        plt.text(x1, y1, s=label, color='white', verticalalignment='top',\n                bbox={'color': color, 'pad': 0})\n        \n    plt.axis('off')\n    plt.show()\n    \nprint(\"Functions ready\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the functions with test images\nNow we're ready to get some predictions from our test images."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "import os\nfrom PIL import Image\n\ntest_dir = \"../../data/object_detection\"\nfor image_file in os.listdir(test_dir):\n    \n    # Load image\n    img_path = os.path.join(test_dir, image_file)\n    image = Image.open(img_path)\n    \n    # Resize image for model input\n    image = letterbox_image(image, tuple(reversed(model_image_size)))\n\n    # Detect objects in the image\n    out_boxes, out_scores, out_classes = detect_objects(image)\n\n    # How many objects did we detect?\n    print('Found {} objects in {}'.format(len(out_boxes), image_file))\n\n    # Display the image with bounding boxes\n    show_objects(image, out_boxes, out_scores, out_classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Acknowledgements and Citations\nThe original YOLO documentation is at https://pjreddie.com/darknet/yolo/.\n\nThe Keras implementation of YOLO used in this exercise is based on the work of qqwweee at https://github.com/qqwweee/keras-yolo3, with some simplifications.\n\nThe test images used in this exercise are from the PASCAL Visual Object Classes Challenge (VOC2007) dataset at http://host.robots.ox.ac.uk/pascal/VOC/voc2007/.\n\n\n    @misc{pascal-voc-2007,\n        author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n        title = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n        howpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}