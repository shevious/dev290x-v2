{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Object Detection with YOLO in PyTorch\n\nIn this exercise, you'll use a PyTorch implementation of YOLO to detect objects in an image.\n\n> **Important**: Using the YOLO model is resource-intensive. before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook.\n\n## Install PyTorch\nFirst, let's ensure that PyTorch is installed.\n\n> *Note: The following `pip install` commands install the CPU-based version of PyTorch on Linux, which is appropriate for the Azure Notebooks environment. For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Install PyTorch\n!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n!pip install torchvision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We'll also use the OpenCV library"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install opencv-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download and Convert YOLO weights\nYOLO is based on the Darknet model architecture - an open-source model written in C. The creators of this model have provided pre-trained weights that were trained on the [Common Objects in Context (COCO) dataset](cocodataset.org) - a common set of sample images for computer vision research.\n\nRun the following cell to download the weights.\n\n> _**Note**: This can take some time to run_"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!wget https://pjreddie.com/media/files/yolov3.weights -O ~/yolov3.weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the Weights into a PyTorch Model\n\nNow that we have the weights, we can load them into a Keras model.\n\n> **Note** The code to implement the Keras model is in **yolo_pytorch/models.py**. Additionally, there are some code files in the **yolo_pytorch/utils** folder that contain functions that are used to assemble and use the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import yolo_pytorch.models as models\nfrom yolo_pytorch.utils.utils import *\n\nimport os\nimport sys\nimport random\n\nimport torch\nimport torchvision.transforms as transforms\n\nprint(\"Using PyTorch\", torch.__version__)\n\n# Set up model\nmodel_config = 'yolo_pytorch/yolov3.cfg'\nimg_size = 416\nhome = os.path.expanduser(\"~\")\nweights = os.path.join(home, \"yolov3.weights\")\n\nmodel = models.Darknet(model_config, img_size)\nmodels.load_darknet_weights(model, weights)\nprint(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the Model to Detect Objects\n\nNow we're ready to use the YOLO model to detect objects in images.\n\n### Create functions to detect and display objects\nWe'll create a couple of functions:\n\n- **detect_objects**: Submits an image to the model and returns predicted object locations\n- **show_objects**: Displays the image with a bounding box fo each detected object."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def detect_objects(model, img):\n    \n    # Use GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n        Tensor = torch.cuda.FloatTensor\n    else:\n        Tensor = torch.FloatTensor\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Get scaled width and height\n    ratio = min(img_size/img.size[0], img_size/img.size[1])\n    imw = round(img.size[0] * ratio)\n    imh = round(img.size[1] * ratio)\n\n    # Transform the image for prediction\n    img_transforms = transforms.Compose([\n         transforms.Resize((imh, imw)),\n         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n                        (128,128,128)),\n         transforms.ToTensor(),\n         ])\n    \n    # convert image to a Tensor\n    image_tensor = img_transforms(img).float()\n    image_tensor = image_tensor.unsqueeze_(0)\n    \n    # Use the model to detect objects in the image\n    with torch.no_grad():\n        detections = model(image_tensor)\n        # Eliminate duplicates with non-max suppression\n        detections = non_max_suppression(detections, 0.8, 0.4)\n    return detections[0]\n\ndef show_objects(img, detections):\n    import random\n    import matplotlib.patches as patches\n    import matplotlib.pyplot as plt\n    \n    # Get bounding-box colors\n    cmap = plt.get_cmap('tab20b')\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    img = np.array(img)\n    plt.figure()\n    fig, ax = plt.subplots(1, figsize=(12,9))\n    ax.imshow(img)\n\n    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n    unpad_h = img_size - pad_y\n    unpad_w = img_size - pad_x\n\n    if detections is not None:\n        # process each instance of each class that was found\n        classes = load_classes('yolo_pytorch/coco.names')\n        unique_labels = detections[:, -1].cpu().unique()\n        n_cls_preds = len(unique_labels)\n        bbox_colors = random.sample(colors, n_cls_preds)\n        # browse detections and draw bounding boxes\n        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n            # Get the class name\n            predicted_class = classes[int(cls_pred)]\n            \n            #We'll display the class name and probability\n            label = '{} {:.2f}'.format(predicted_class, cls_conf)\n            \n            # Set the box dimensions\n            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n            \n            # Add a box with the color for this class\n            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n            ax.add_patch(bbox)\n            plt.text(x1, y1, s=label, color='white', verticalalignment='top',\n                    bbox={'color': color, 'pad': 0})\n    plt.axis('off')\n\n    plt.show()\n\nprint(\"Functions ready\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the functions with test images\nNow we're ready to get some predictions from our test images."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "import os\nfrom PIL import Image\n\ntest_dir = \"../../data/object_detection\"\nfor image_file in os.listdir(test_dir):\n    \n    # Load image\n    img_path = os.path.join(test_dir, image_file)\n    image = Image.open(img_path)\n\n    # Detect objects in the image\n    detections = detect_objects(model, image)\n\n    # How many objects did we detect?\n    print('Found {} objects in {}'.format(len(detections), image_file))\n\n    # Display the image with bounding boxes\n    show_objects(image, detections)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Acknowledgements and Citations\nThe original YOLO documentation is at https://pjreddie.com/darknet/yolo/.\n\nThe PyTorch implementation of YOLO used in this exercise is based on the work of Ultralytics at https://github.com/ultralytics/yolov3, with some simplifications.\n\nThe test images used in this exercise are from the PASCAL Visual Object Classes Challenge (VOC2007) dataset at http://host.robots.ox.ac.uk/pascal/VOC/voc2007/.\n\n\n    @misc{pascal-voc-2007,\n        author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n        title = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n        howpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}