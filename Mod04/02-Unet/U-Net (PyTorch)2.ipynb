{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Semantic Segmentation with PyTorch\n\nIn this exercise, you'll use the U-Net network to perform binary classification and segmentation for images of planes.\n\n> **Important**: Using the U-Net model is resource-intensive. before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Install PyTorch\n\nTo begin with, we'll install the latest version of PyTorch, and import the libraries we need.\n\n> *Note: The following `pip install` commands install the CPU-based version of PyTorch on Linux, which is appropriate for the Azure Notebooks environment. For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install PyTorch\n!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n!pip install torchvision\n\n# Import PyTorch libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as td\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Import the U-Net Code\n\nThe code to implement U-Net is provided in the **model.py** python file.\n\n> **Tip**: You should explore the code in this file to get a better understanding of the way the model works.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from unet_pytorch.model import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Explore the Training Data\n\nThe training data for a U-Net model consists of two kinds of input:\n\n- **Image files**: The images that represent the *features* on which we want to train the model.\n- **Mask files**: Images of the object masks that the network will be trained to predict - these are the *labels*.\n\nIn this example, we're going to use U-Net for binary classification of airplanes images, so there's only one class of object - and therefore one class of mask. We've deliberately made this example as simple as possible, partly to make it easier to understand what's going on, and partly to ensure it can be run in a resource-constrained environment. \n\nLet's take a look at the training images and masks:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom matplotlib import pyplot as plt\nimport skimage.io as io\nimport numpy as np\n%matplotlib inline\n\n\nfig = plt.figure(figsize=(12, 60))\n\ntrain_dir = '../../data/segmentation/train'\nimage_dir = os.path.join(train_dir,\"image/plane\")\nmask_dir = os.path.join(train_dir,\"mask/plane\")\n\nfiles = os.listdir(image_dir)\nrows = len(files)\ncell = 0\nfor file in files:\n    cell += 1\n    \n    # Open the image and mask files\n    img_path = os.path.join(image_dir, file)\n    img = io.imread(img_path, as_gray = True)\n    \n    mask_path = os.path.join(mask_dir, file)\n    mask = io.imread(mask_path, as_gray = True)\n    \n    # plot the image\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    cell += 1\n\n    # plot the mask\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(mask, \"gray\")\n    cell += 1\n    \n    # Plot them overlaid\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    imgplot=plt.imshow(mask, \"gray\", alpha=0.4)\n\nplt.show()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the Training Data\n\nTo load the images, we'll define a custom PyTorch dataset that gets each image and its corresponding mask.\n\nWe have a very small number of training images, so we'll apply some data augmentation to randomly flip the images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class PlaneDataSet(td.Dataset):\n    \n    def __init__(self, image_path, mask_path):\n        import os\n        \n        self.image_path = image_path\n        self.mask_path = mask_path\n        self.filenames = os.listdir(mask_path)\n        \n    def __getitem__(self, index):\n        import os\n        import random\n        import numpy as np\n        from PIL import Image\n        import skimage.io as io\n        \n        # Get a random intenger between 1 and 2\n        rand = random.randint(0, 2)     \n        \n        #Load the image\n        img_path = os.path.join(self.image_path, self.filenames[index])\n        image =  io.imread(img_path, as_gray = True)\n        \n        # Normalize the image pixels between 0 and 1\n        image_np = image/255\n        \n        # Randomly flip the image half the time\n        if rand > 1:\n            image_pil = Image.fromarray(image_np).transpose(Image.FLIP_LEFT_RIGHT).convert(\"L\")          \n            image_np = np.array(image_pil)\n        \n        # Add a dimension and convert to a tensor\n        image_np = np.expand_dims(image_np, axis=0)\n        image_tensor = torch.Tensor(image_np)\n        \n        # Now do all the same stuff to the mask image\n        mask_path = os.path.join(self.mask_path, self.filenames[index])\n        mask =  io.imread(mask_path, as_gray = True)\n        mask_np = mask/255\n        if rand > 1:\n            mask_pil = Image.fromarray(mask_np).transpose(Image.FLIP_LEFT_RIGHT).convert(\"L\")            \n            mask_np = np.array(mask_pil)\n        mask_np = np.expand_dims(mask_np, axis=0)\n        mask_tensor = torch.Tensor(mask_np)\n        \n        return(image_tensor, mask_tensor)\n    \n    def __len__(self):\n        return len(self.filenames)\n                           \n     ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can use our dataset to create a data loader that gets batches of training data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to ingest data\ndef load_data(image_path, mask_path):\n\n    # Load all of the images, transforming them\n    dataset = PlaneDataSet(image_path , mask_path)\n    \n    # define a loader for the image data\n    image_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=2,\n        num_workers=0,\n        shuffle=True\n    )\n    \n        \n    return image_loader\n\nimage_loader = load_data(image_dir, mask_dir)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download the Model Weights\nThe model has already been partially trained, so we'll download the trained weights as a starting point."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!wget \"https://aka.ms/unet-pt\" -O ~/unet.pt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the Model\n\nNow we're ready to train the U-Net model. We'll train it from the data loader we created.\n\nAfter training is complete, we'll save the model weights.\n\n> _**Note**: This will take a while on a non-GPU machine - go get some coffee!_"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(model, device, data_loader, optimizer, epoch):\n    \n    model.to(device)\n    model.train()\n    \n    train_loss = 0\n    print(\"Epoch:\", epoch, \"...\")\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(data_loader):\n        \n        \n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n        \n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n    # return average loss for the epoch\n    avg_training_loss = train_loss / (batch_idx+1)\n    print(\"\\tTraining set: Average loss: {:.6f}\".format(avg_training_loss))\n    return avg_training_loss\n\n# Create a UNet model\nmodel = UNet(n_channels=1, n_classes = 1)\n\n# Use the best available device (GPU/CPU) for training\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the weights\nhome = os.path.expanduser(\"~\")\nweights_file = os.path.join(home, \"unet.pt\")\nmodel.load_state_dict(torch.load(weights_file, map_location=device))\n\n# Specify the optimizer and  loss criteria\noptimizer = optim.RMSprop(model.parameters(), lr=0.001)\nloss_criteria = nn.BCELoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\n\nepochs = 2\nprint('Training on', device)\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, image_loader, optimizer, epoch)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n\n# Save the model weights\ntorch.save(model.state_dict(), weights_file)\nprint(\"Model saved.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test the Trained Model\n\nOK, let's see how well our trained model does with some images of airplanes it hasn't seen."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom matplotlib import pyplot as plt\nimport skimage.io as io\nfrom unet_pytorch.model import *\nimport numpy as np\n%matplotlib inline\n\n#Create a new instance of the model and load the saved weights\nmodel = UNet(n_channels=1, n_classes = 1)\nhome = os.path.expanduser(\"~\")\nweights_file = os.path.join(home, \"unet.pt\")\nmodel.load_state_dict(torch.load(weights_file, map_location='cpu'))\nmodel.eval()\n\nfig = plt.figure(figsize=(12, 60))\n\ntest_dir = '../../data/segmentation/test'\n\nfiles = os.listdir(test_dir)\nrows = len(files)\ncell = 0\nfor file in files:\n    cell += 1\n    # Open the file\n    img_path = os.path.join(test_dir, file)\n    img =  io.imread(img_path, as_gray = True)\n    \n    # Normalize and convert to a tensor array\n    image_tensor = torch.stack([torch.Tensor(img/255).unsqueeze(0)])\n    \n    # Plot the original image\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    cell += 1\n    \n    # Get the predicted mask and plot it\n    with torch.no_grad():\n        mask_predictions = model(image_tensor)\n    mask = mask_predictions.data.numpy()[0]\n    img_mask = mask[0]\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img_mask, \"gray\")\n    cell += 1\n    \n    # Plot the mask overlaid on the image\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    imgplot=plt.imshow(img_mask, \"binary\", alpha=0.7)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It's not fantastic, largely because we used such a small amount of data; but hopefully it serves to demonstrate the principles of semantic segmentation with U-Net.\n\n## Acknowledgements and Citations\n\nThe U-Net architecture is documented by its inventors (Olaf Ronneberger, Philipp Fischer, and Thomas Brox), at https://arxiv.org/abs/1505.04597.\n\nThe PyTorch implementation of U-Net used in this exercise is based on milesial's work at https://github.com/milesial/Pytorch-UNet, with some simplifications. \n\nThe data used in this exercise includes images adapted from the PASCAL Visual Object Classes Challenge (VOC2007) dataset at http://host.robots.ox.ac.uk/pascal/VOC/voc2007/.\n\n\n    @misc{pascal-voc-2007,\n        author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n        title = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n        howpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}