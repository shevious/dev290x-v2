{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Semantic Segmentation with Keras\n\nIn this exercise, you'll use the U-Net network to perform binary classification and segmentation for images of planes.\n\n> **Important**: Using the U-Net model is resource-intensive. before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Install Keras\n\nTo begin with, we'll install the latest version of Keras."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Explore the Training Data\n\nThe training data for a U-Net model consists of two kinds of input:\n\n- **Image files**: The images that represent the *features* on which we want to train the model.\n- **Mask files**: Images of the object masks that the network will be trained to predict - these are the *labels*.\n\nIn this example, we're going to use U-Net for binary classification of airplanes images, so there's only one class of object - and therefore one class of mask. We've deliberately made this example as simple as possible, partly to make it easier to understand what's going on, and partly to ensure it can be run in a resource-constrained environment. \n\nLet's take a look at the training images and masks:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom matplotlib import pyplot as plt\nimport skimage.io as io\nimport numpy as np\n%matplotlib inline\n\n\nfig = plt.figure(figsize=(12, 60))\n\n\ntrain_dir = '../../data/segmentation/train'\nimage_dir = os.path.join(train_dir,\"image/plane\")\nmask_dir = os.path.join(train_dir,\"mask/plane\")\n\nfiles = os.listdir(image_dir)\nrows = len(files)\ncell = 0\nfor file in files:\n    cell += 1\n    \n    # Open the image and mask files\n    img_path = os.path.join(image_dir, file)\n    img = io.imread(img_path, as_gray = True)\n    \n    mask_path = os.path.join(mask_dir, file)\n    mask = io.imread(mask_path, as_gray = True)\n    \n    # plot the image\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    cell += 1\n\n    # plot the mask\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(mask, \"gray\")\n    cell += 1\n    \n    # Plot them overlaid\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img, \"gray\")\n    imgplot=plt.imshow(mask, \"gray\", alpha=0.4)\n\nplt.show()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Import the U-Net Code\n\nThe code to implement U-Net is provided in two python files:\n\n- **model.py**: This file contains the code that implements the U-Net model\n- **data.py**: This file contains functions to help load and prepare training data.\n\n> **Tip**: You should explore the code in these files to get a better understanding of the way the model works.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from unet_keras.data import *\nfrom unet_keras.model import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The ouput from the code above shows the training images with their corresponding mask labels, and finally the mask overlaid on the image so you can clearly see that the masks represent the pixels that belong to the plane objects in the images.\n\n> **Note**: We deliberately chose images in which the plane objects are clearly contrasted with the background to make it easier to train with a very small number of training images and a very small amount of training!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the Training Data\nWe have a very small number of training images, so we'll apply some data augmentation to randomly flip, zoom, shear, and otherwise transform the images for each batch."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\ntrain_images = trainGenerator(2,train_dir,'image','mask',data_gen_args,save_to_dir = None)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download the Model Weights\nThe model has already been partially trained, so we'll download the trained weights as a starting point."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!wget \"https://aka.ms/unet.h5\" -O ~/unet.h5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the Model\n\nNow we're ready to train the U-Net model. We'll train it from the training generator we created, and save the model weights after each epoch if the loss has improved. In this example, to reduce the required compute resources we'll train it for just one epoch with minimal batches. In reality, you'd need to train the model over several epochs on a GPU-based computer.\n\n> _**Note**: This will take a while on a non-GPU machine - go get some coffee!_"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = unet()\nhome = os.path.expanduser(\"~\")\nweights_file = os.path.join(home, \"unet.h5\")\nmodel.load_weights(weights_file)\nmodel_checkpoint = ModelCheckpoint(weights_file, monitor='loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_images,steps_per_epoch=1,epochs=1,callbacks=[model_checkpoint])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test the Trained Model\n\nOK, let's see how well our trained model does with some images of airplanes it hasn't seen."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom matplotlib import pyplot as plt\nimport skimage.io as io\nimport numpy as np\n%matplotlib inline\n\nmodel = unet()\nmodel.load_weights(weights_file)\n\nfig = plt.figure(figsize=(12, 60))\n\n\ntest_dir = '../../data/segmentation/test'\n\nfiles = os.listdir(test_dir)\nrows = len(files)\ncell = 0\nfor file in files:\n    cell += 1\n    # Open the file\n    img_path = os.path.join(test_dir, file)\n    img = io.imread(img_path, as_gray = True)\n    \n    src_img = img\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(src_img, \"gray\")\n    cell += 1\n\n    img = np.reshape(img,img.shape+(1,))\n    mask_predictions = model.predict([[img]])\n    mask = mask_predictions[0]\n    img_mask = mask.reshape(mask.shape[0], mask.shape[1])\n\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(img_mask, \"gray\")\n    cell += 1\n\n\n    a=fig.add_subplot(rows,3,cell)\n    imgplot=plt.imshow(src_img, \"gray\")\n    imgplot=plt.imshow(img_mask, \"binary\", alpha=0.6)\n\nplt.show()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It's not fantastic, largely because we used such a small amount of data; but hopefully it serves to demonstrate the principles of semantic segmentation with U-Net.\n\n## Acknowledgements and Citations\n\nThe U-Net architecture is documented by its inventors (Olaf Ronneberger, Philipp Fischer, and Thomas Brox), at https://arxiv.org/abs/1505.04597.\n\nThe Keras implementation of U-Net used in this exercise is based on zhixuhao's work at https://github.com/zhixuhao/unet, with some simplifications. \n\nThe data used in this exercise includes images adapted from the PASCAL Visual Object Classes Challenge (VOC2007) dataset at http://host.robots.ox.ac.uk/pascal/VOC/voc2007/.\n\n\n    @misc{pascal-voc-2007,\n        author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n        title = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n        howpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}