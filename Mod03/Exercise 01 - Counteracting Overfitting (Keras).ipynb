{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Counteracting Overfitting\n\nOverfitting is the primary cause of model innacuracy. When a model is overfitted, it performs well when predicting the class of images on which it has been trained; but does not generalize well to new images.\n\n## Techniques for Avoiding Overfitting\n\nThere are a number of ways to address overfitting during the training process. In this notebook, we'll look at two of the most common approaches.\n\n### Dropping Feature Maps\n\nThe first approach is somewhat counter-intuitive, but very effective. During the training process, the convolution and pooling layers in the feature extraction section of the model generate lots of feature maps from the training images. Randomly dropping some of these feature maps helps vary the features that are extracted in each batch, ensuring the model doesn't become overly-reliant on any one dominant feature in the training data.\n\n### Data Augmentation\n\nIn an ideal world, you'd have a huge volume of training data that is representative of all future data that you will submit to the model for inference. In reality, you must often traing a model with a limited set of training data, which can exacerbate the overfitting problem. One way to mitigate this is to perform data augmentation by making random transformations of the training images; for example by flipping, rotating, or cropping them. Because you randomly apply these data augmentation transformations during training, the same image might be presented differently from batch to batch, creating more variation in the training data and helping the model to learn features based the same objects at different orientations or scales."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Adding Drop Layers and Data Augementation to a CNN\n\nLet's take a look at using these techniques when training a Keras model. First, we'll import the latest version of Keras and prepare to load our training data."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras\n\nimport sys\nimport keras\nfrom keras import backend as K\nimport os\n\nprint('Keras version:',keras.__version__)\n\n# The images are in a folder named 'shapes/training'\ntraining_folder_name = '../data/shapes/training'\n\n# All images are 128x128 pixels\nimg_size = (128,128)\n\n# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Augment the Data\nNow we're ready to define our data loaders. At this point we can add transformations to randomly modify the images as they are added to a training batch. In this case, we'll flip images horizontally at random."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.image import ImageDataGenerator\n\nbatch_size = 30\n\nprint(\"Getting Data...\")\ndatagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n                             horizontal_flip=True, # Flip some images at random\n                             validation_split=0.3) # hold back 30% of the images for validation\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow_from_directory(\n    training_folder_name,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow_from_directory(\n    training_folder_name,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Add Drop Layers to the CNN\nNow we're ready to define our model, which will include some drop layers to randomly drop some of the extracted features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define a CNN classifier network\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout\nfrom keras.layers import Activation, Flatten, Dense\nfrom keras import optimizers\n\n# Define the model as a sequence of layers\nmodel = Sequential()\n\n# The input layer accepts an image and applies a convolution that uses 32 6x6 filters and a rectified linear unit activation function\nmodel.add(Conv2D(32, (6, 6), input_shape=train_generator.image_shape, activation='relu'))\n\n# Next we'll add a max pooling layer with a 2x2 patch\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n# We can add as many layers as we think necessary - here we'll add another convolution layer and another and max poolinglayer\nmodel.add(Conv2D(32, (6, 6), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# A dropout layer randomly drops some features to reduce inter-dependencies (which can cause over-fitting)\nmodel.add(Dropout(0.3))\n\n# Now we'll flatten the feature maps and generate an output layer with a predicted probability for each class\nmodel.add(Flatten())\nmodel.add(Dense(train_generator.num_classes, activation='softmax'))\n\n# We'll use the ADAM optimizer\nopt = optimizers.Adam(lr=0.001)\n\n# With the layers defined, we can now compile the model for categorical (multi-class) classification\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train the Model\n\nWith the layers of the CNN defined, we're ready to train the model using our randomly augmented image data. Since we're dropping some features, it may require more epochs to get the loss to drop so that the model is reasonably accurate - but the data augmentation and dropped features should help ensure that as we train for more epochs, the validation loss drops along with the training loss; meaning that the model will generalize well."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train the model over 10 epochs\nnum_epochs = 10\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = num_epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the Loss History\nWe tracked average training and validation loss history for each epoch. We can plot these to verify that loss reduced as the model was trained, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nepoch_nums = range(1,num_epochs+1)\ntraining_loss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save the Model\nNow that we have trained the model, we can save it with the trained weights."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\n\nmodelFileName = 'shape-classifier.h5'\n\nmodel.save(modelFileName) # saves the trained model\nprint(\"Model saved.\")\n\ndel model  # deletes the existing model variable",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the Model with New Data\nNow that we've trained and evaluated our model, we can use it to predict classes for new images.\n\n### Create Functions to Prepare Data and Get Class Predictions\nLet's create a couple of functions to:\n\n- Resize new images to match the size on which the model was trained.\n- Submit the new images to the model and retrieve the predicted classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Helper function to resize image\ndef resize_image(src_img, size=(128,128), bg_color=\"white\"): \n    from PIL import Image\n\n    # rescale the image so the longest edge is the right size\n    src_img.thumbnail(size, Image.ANTIALIAS)\n    \n    # Create a new image of the right shape\n    new_image = Image.new(\"RGB\", size, bg_color)\n    \n    # Paste the rescaled image onto the new background\n    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n  \n    # return the resized image\n    return new_image\n\n# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n    import numpy as np\n    \n    # We need to format the input to match the training data\n    # The data generator loaded the values as floating point numbers\n    # and normalized the pixel values, so...\n    img_features = image_array.astype('float32')\n    img_features /= 255\n    \n    # These are the classes our model can predict\n    classnames = ['circle', 'square', 'triangle']\n    \n    # Predict the class of each input image\n    predictions = classifier.predict(img_features)\n    \n    predicted_classes = []\n    for prediction in predictions:\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(classnames[int(class_idx)])\n    # Return the predictions\n    return predicted_classes\n\nprint(\"Functions created - ready to use model for inference.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predict Image Classes\nNow we're ready to use the model for predicting (often referred to as *inferencing*) the classes of some new images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom random import randint\nimport numpy as np\nfrom PIL import Image\nfrom keras.models import load_model\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# load the saved model\nmodelFileName = 'shape-classifier.h5'\nmodel = load_model(modelFileName) \n\n#get the list of test image files\ntest_folder = '../data/shapes/test'\ntest_image_files = os.listdir(test_folder)\n\n# Empty array on which to store the images\nimage_arrays = []\n\nsize = (128,128)\nbackground_color=\"white\"\n\nfig = plt.figure(figsize=(12, 8))\n\n# Get the images and show the predicted classes\nfor file_idx in range(len(test_image_files)):\n    img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n    \n    # resize the image so it matches the training set - it  must be the same size as the images on which the model was trained\n    resized_img = np.array(resize_image(img, size, background_color))\n                      \n    # Add the image to the array of images\n    image_arrays.append(resized_img)\n\n# Get predictions from the array of image arrays\n# Note that the model expects an array of 1 or more images - just like the batches on which it was trained\npredictions = predict_image(model, np.array(image_arrays))\n\n# plot easch image with its corresponding prediction\nfor idx in range(len(predictions)):\n    a=fig.add_subplot(1,len(predictions),idx+1)\n    imgplot = plt.imshow(image_arrays[idx])\n    a.set_title(predictions[idx])\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}