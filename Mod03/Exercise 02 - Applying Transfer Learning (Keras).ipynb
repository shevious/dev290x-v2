{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Transfer Learning\nA Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n\n<table>\n    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n</table>\n\n*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n\nHow does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners.\n\nIn this notebook, we'll see how to implement transfer learning for a classification model.\n\n> **Important**:The base model used in this exercise is large, and training is resource-intensive. Before running the code in this notebook, shut down all other notebooks in this library (In each open notebook other than this one, on the **File** menu, click **Close and Halt**). If you experience and Out-of-Memory (OOM) error when running code in this notebook, shut down this entire library, and then reopen it and open only this notebook.\n\n## Using Transfer Learning to Train a CNN\n\nFirst, we'll import the latest version of Keras and prepare to load our training data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade keras\n\nimport keras\nfrom keras import backend as K\n\nprint('Keras version:',keras.__version__)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prepare the Data\nBefore we can train the model, we need to prepare the data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# The images are in a folder named 'shapes/training'\ntraining_folder_name = '../data/shapes/training'\n\n# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)\n\n# Our source images are 128x128, but the base model we're going to use was trained with 224x224 images\npretrained_size = (224,224)\nbatch_size = 15\n\nprint(\"Getting Data...\")\ndatagen = ImageDataGenerator(rescale=1./255, # normalize pixel values\n                             validation_split=0.3) # hold back 30% of the images for validation\n\nprint(\"Preparing training dataset...\")\ntrain_generator = datagen.flow_from_directory(\n    training_folder_name,\n    target_size=pretrained_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nprint(\"Preparing validation dataset...\")\nvalidation_generator = datagen.flow_from_directory(\n    training_folder_name,\n    target_size=pretrained_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Download a trained model to use as a base\nThe VGG16 model is an image classifier that was trained on the ImageNet dataset - a huge dataset containing thousands of images of many kinds of object. We'll download the trained model, excluding its top layer, and set its input shape to match our image data.\n\n*Note: The **keras.applications** namespace includes multiple base models, some which may perform better for your dataset than others. We've chosen this model because it's fairly lightweight within the limited resources of the Azure Notebooks environment.*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras import applications\n#Load the base model, not including its final connected layer, and set the input shape to match our images\nbase_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=train_generator.image_shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Freeze the already trained layers and add a custom output layer for our classes\nThe existing feature extraction layers are already trained, so we just need to add a couple of layers so that the model output is the predictions for our classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras import Model\nfrom keras.layers import Flatten, Dense\nfrom keras import optimizers\n\n# Freeze the already-trained layers in the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Create layers for classification of our images\nx = base_model.output\nx = Flatten()(x)\nprediction_layer = Dense(len(classes), activation='sigmoid')(x) \nmodel = Model(inputs=base_model.input, outputs=prediction_layer)\n\n# Compile the model\nopt = optimizers.Adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\n# Now print the full model, which will include the layers of the base model plus the dense layer we added\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train the Model\nWith the layers of the CNN defined, we're ready to train the top layer using our image data. This will take a considerable amount of time on a CPU due to the complexity of the base model, so we'll train the model over only one epoch."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train the model over 1 epoch using 15-image batches and using the validation holdout dataset for validation\nnum_epochs = 1\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples // batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples // batch_size,\n    epochs = num_epochs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Using the Trained Model\nNow that we've trained the model, we can use it to predict the class of an image."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Helper function to resize image\ndef resize_image(src_img, size=(128,128), bg_color=\"white\"): \n    from PIL import Image\n\n    # rescale the image so the longest edge is the right size\n    src_img.thumbnail(size, Image.ANTIALIAS)\n    \n    # Create a new image of the right shape\n    new_image = Image.new(\"RGB\", size, bg_color)\n    \n    # Paste the rescaled image onto the new background\n    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n  \n    # return the resized image\n    return new_image\n\n# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n    import numpy as np\n    \n    # We need to format the input to match the training data\n    # The data generator loaded the values as floating point numbers\n    # and normalized the pixel values, so...\n    img_features = image_array.astype('float32')\n    img_features /= 255\n    \n    # These are the classes our model can predict\n    classnames = ['circle', 'square', 'triangle']\n    \n    # Predict the class of each input image\n    predictions = classifier.predict(img_features)\n    \n    predicted_classes = []\n    for prediction in predictions:\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(classnames[int(class_idx)])\n    # Return the predictions\n    return predicted_classes\n\nprint(\"Functions created - ready to use model for inference.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom random import randint\nimport numpy as np\nfrom PIL import Image\nfrom keras.models import load_model\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n#get the list of test image files\ntest_folder = '../data/shapes/test'\ntest_image_files = os.listdir(test_folder)\n\n# Empty array on which to store the images\nimage_arrays = []\n\nsize = (224,224)\nbackground_color=\"white\"\n\nfig = plt.figure(figsize=(12, 8))\n\n# Get the images and show the predicted classes\nfor file_idx in range(len(test_image_files)):\n    img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n    \n    # resize the image so it matches the training set - it  must be the same size as the images on which the model was trained\n    resized_img = np.array(resize_image(img, size, background_color))\n                      \n    # Add the image to the array of images\n    image_arrays.append(resized_img)\n\n# Get predictions from the array of image arrays\n# Note that the model expects an array of 1 or more images - just like the batches on which it was trained\npredictions = predict_image(model, np.array(image_arrays))\n\n# plot easch image with its corresponding prediction\nfor idx in range(len(predictions)):\n    a=fig.add_subplot(1,len(predictions),idx+1)\n    imgplot = plt.imshow(image_arrays[idx])\n    a.set_title(predictions[idx])\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}