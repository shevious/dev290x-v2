{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Counteracting Overfitting\n\nOverfitting is the primary cause of model innacuracy. When a model is overfitted, it performs well when predicting the class of images on which it has been trained; but does not generalize well to new images.\n\n## Techniques for Avoiding Overfitting\n\nThere are a number of ways to address overfitting during the training process. In this notebook, we'll look at two of the most common approaches.\n\n### Dropping Feature Maps\n\nThe first approach is somewhat counter-intuitive, but very effective. During the training process, the convolution and pooling layers in the feature extraction section of the model generate lots of feature maps from the training images. Randomly dropping some of these feature maps helps vary the features that are extracted in each batch, ensuring the model doesn't become overly-reliant on any one dominant feature in the training data.\n\n### Data Augmentation\n\nIn an ideal world, you'd have a huge volume of training data that is representative of all future data that you will submit to the model for inference. In reality, you must often traing a model with a limited set of training data, which can exacerbate the overfitting problem. One way to mitigate this is to perform data augmentation by making random transformations of the training images; for example by flipping, rotating, or cropping them. Because you randomly apply these data augmentation transformations during training, the same image might be presented differently from batch to batch, creating more variation in the training data and helping the model to learn features based the same objects at different orientations or scales."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Adding Drop Layers and Data Augementation to a CNN\n\nLet's take a look at using these techniques when training a PyTorch model. First, we'll import the latest version of PyTorch and prepare to load our training data.\n\n> *Note: The following `pip install` commands install the CPU-based version of PyTorch on Linux, which is appropriate for the Azure Notebooks environment. For instructions on how to install the PyTorch and TorchVision packages on your own system, see https://pytorch.org/get-started/locally/*"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install PyTorch\n!pip install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n!pip install torchvision\n\n# Import PyTorch libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Augment the Data\nNow we're ready to define our data loaders. At this point we can add transformations to randomly modify the images as they are added to a training batch. In this case, we'll flip images horizontally at random."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to ingest data using training and test loaders\ndef load_dataset(data_path):\n    # Load all of the images\n    transformation = transforms.Compose([\n        # Randomly augment the image data\n        transforms.RandomHorizontalFlip(0.5),\n        # transform to tensors\n        transforms.ToTensor(),\n        # Normalize the pixel values (in R, G, and B channels)\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Load all of the images, transforming them\n    full_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        transform=transformation\n    )\n    \n    \n    # Split into training (70% and testing (30%) datasets)\n    train_size = int(0.7 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n    \n    # define a loader for the training data we can iterate through in 50-image batches\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # define a loader for the testing data we can iterate through in 50-image batches\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=50,\n        num_workers=0,\n        shuffle=False\n    )\n        \n    return train_loader, test_loader\n\nimport os\n\n# The images are in a folder named 'shapes/training'\ntraining_folder_name = '../data/shapes/training'\n\n# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = load_dataset(training_folder_name)\nbatch_size = train_loader.batch_size\nprint(\"Data loaders ready to read\", training_folder_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Add Drop Layers to the CNN\nNow we're ready to define our model, which will include some drop layers to randomly drop some of the extracted features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a neural net class\nclass Net(nn.Module):\n    # Constructor\n    def __init__(self, num_classes=3):\n        super(Net, self).__init__()\n        \n        # Our images are RGB, so input channels = 3. We'll apply 12 filters in the first convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n        \n         # A second convolutional layer takes 12 input channels, and generates 24 outputs\n        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n        \n        # We'll apply max pooling with a kernel size of 2\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        \n        # A drop layer deletes 30% of the features to help prevent overfitting\n        self.drop = nn.Dropout2d(p=0.3)\n        \n        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n        # So our feature tensors are now 32 x 32, and we've generated 24 of them\n        # We need to flatten these and feed them to a fully-connected layer\n        # to map them to  the probability for each class\n        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n\n    def forward(self, x):\n        # Use a relu activation function after layer 1 (convolution 1 and pool)\n        x = F.relu(self.pool(self.conv1(x)))\n        \n        # Use a relu activation function after layer 2\n        x = F.relu(self.pool(self.conv2(x)))\n        \n        # Select some features to drop to prevent overfitting (only drop during training)\n        x = F.dropout(self.drop(x), training=self.training)\n        \n        # Flatten\n        x = x.view(-1, 32 * 32 * 24)\n        # Feed to fully-connected layer to predict class\n        x = self.fc(x)\n        # Return class probabilities via a log_softmax function \n        return torch.log_softmax(x, dim=1)\n    \ndevice = \"cpu\"\nif (torch.cuda.is_available()):\n    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n    device = \"cuda\"\n\n# Create an instance of the model class and allocate it to the device\nmodel = Net(num_classes=len(classes)).to(device)\n\nprint(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train the Model\n\nWith the layers of the CNN defined, we're ready to train the model using our randomly augmented image data. Since we're dropping some features, it may require more epochs to get the loss to drop so that the model is reasonably accurate - but the data augmentation and dropped features should help ensure that as we train for more epochs, the validation loss drops along with the training loss; meaning that the model will generalize well."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Use the CPU or GPU as appropriate\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n\n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics so we see some progress\n        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n            \n   # return average loss for the epoch\n    avg_loss = train_loss / (batch_idx+1)\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss\n            \n            \ndef test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            \n            # Get the predicted classes for this batch\n            output = model(data)\n            \n            # Calculate the loss for this batch\n            test_loss += loss_criteria(output, target).item()\n            \n            # Calculate the accuracy for this batch\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target==predicted).item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    avg_loss = test_loss/batch_count\n    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        avg_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return avg_loss\n    \n    \n# Use an \"Adam\" optimizer to adjust weights\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\n# Train over 10 epochs\nepochs = 10\nprint('Training on', device)\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, train_loader, optimizer, epoch)\n        test_loss = test(model, device, test_loader)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n        validation_loss.append(test_loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View the Loss History\nWe tracked average training and validation loss for each epoch. We can plot these to verify that loss reduced as the model was trained, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save the Model\nNow that we have trained the model, we can save its weights. Then later, we can reload those weights into an instance of the same network and use it to predict classes from new images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Save the model weights\nmodel_file = 'shape-classifier.pt'\ntorch.save(model.state_dict(), model_file)\nprint(\"Model saved.\")\n\n# Delete the existing model variable\ndel model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the Model with New Data\nNow that we've trained and evaluated our model, we can use it to predict classes for new images.\n\n### Create Functions to Prepare Data and Get Class Predictions\nLet's create a couple of functions to:\n\n- Resize new images to match the size on which the model was trained.\n- Submit the new images to the model and retrieve the predicted classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Helper function to resize image\ndef resize_image(src_img, size=(128,128), bg_color=\"white\"): \n    from PIL import Image\n\n    # rescale the image so the longest edge is the right size\n    src_img.thumbnail(size, Image.ANTIALIAS)\n    \n    # Create a new image of the right shape\n    new_image = Image.new(\"RGB\", size, bg_color)\n    \n    # Paste the rescaled image onto the new background\n    new_image.paste(src_img, (int((size[0] - src_img.size[0]) / 2), int((size[1] - src_img.size[1]) / 2)))\n  \n    # return the resized image\n    return new_image\n\n# Function to predict the class of an image\ndef predict_image(classifier, image_array):\n   \n    # Set the classifer model to evaluation mode\n    classifier.eval()\n    \n    # These are the classes our model can predict\n    class_names = ['circle', 'square', 'triangle']\n    \n    # Apply the same transformations as we did for the training images\n    transformation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\n    # Preprocess the imagees\n    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n\n    # Turn the input into a Variable\n    input_features = image_tensor\n\n    # Predict the class of each input image\n    predictions = classifier(input_features)\n    \n    predicted_classes = []\n    # Convert the predictions to a numpy array \n    for prediction in predictions.data.numpy():\n        # The prediction for each image is the probability for each class, e.g. [0.8, 0.1, 0.2]\n        # So get the index of the highest probability\n        class_idx = np.argmax(prediction)\n        # And append the corresponding class name to the results\n        predicted_classes.append(class_names[class_idx])\n    return np.array(predicted_classes)\n\nprint(\"Functions created - ready to use model for inference.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predict Image Classes\nNow we're ready to use the model for predicting (often referred to as *inferencing*) the classes of some new images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom random import randint\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# load the saved model weights\nmodel = Net()\nmodel.load_state_dict(torch.load(model_file))\n\n\n#get the list of test image files\ntest_folder = '../data/shapes/test'\ntest_image_files = os.listdir(test_folder)\n\n# Empty array on which to store the images\nimage_arrays = []\n\nsize = (128,128)\nbackground_color=\"white\"\n\nfig = plt.figure(figsize=(12, 8))\n\n# Get the images and show the predicted classes\nfor file_idx in range(len(test_image_files)):\n    img = Image.open(os.path.join(test_folder, test_image_files[file_idx]))\n    \n    # resize the image so it matches the training set - it  must be the same size as the images on which the model was trained\n    resized_img = np.array(resize_image(img, size, background_color))\n                      \n    # Add the image to the array of images\n    image_arrays.append(resized_img)\n\n# Get predictions from the array of image arrays\n# Note that the model expects an array of 1 or more images - just like the batches on which it was trained\npredictions = predict_image(model, np.array(image_arrays))\n\n# plot easch image with its corresponding prediction\nfor idx in range(len(predictions)):\n    a=fig.add_subplot(1,len(predictions),idx+1)\n    imgplot = plt.imshow(image_arrays[idx])\n    a.set_title(predictions[idx])\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}